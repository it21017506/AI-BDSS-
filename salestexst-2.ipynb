{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install shap matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3RWRR5b0Zq3",
        "outputId": "0e5342f9-8bba-489c-f09b-c0e14e5b4418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKIqxqha7KjJ",
        "outputId": "b9ff4a14-ea4c-43bd-c13e-50df60bf1ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install required packages\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install plotly\n",
        "!pip install tensorflow\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1YkEnS07-02",
        "outputId": "09fb4913-d142-477b-caaa-d9fc35705751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.48.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.48.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.48.1 watchdog-6.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (25.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ngrok (a tunneling service) and set it up\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3gFYzl_8AzC",
        "outputId": "9558a685-b310-42a9-d90a-7972b2b5d40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-27 08:12:33--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 75.2.60.68, 13.248.244.96, 99.83.220.108, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|75.2.60.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‚Äòngrok-stable-linux-amd64.zip‚Äô\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  66.3MB/s    in 0.2s    \n",
            "\n",
            "2025-07-27 08:12:33 (66.3 MB/s) - ‚Äòngrok-stable-linux-amd64.zip‚Äô saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3E3xbV87k4H",
        "outputId": "c6fdba57-2914-4634-aad9-b2453a84e157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app4.py\n"
          ]
        }
      ],
      "source": [
        "# First, install required packages\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n",
        "!pip install plotly\n",
        "!pip install tensorflow\n",
        "\n",
        "\n",
        "# Save your Streamlit app to a file\n",
        "%%writefile app4.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Advanced Sales Prediction AI\",\n",
        "    page_icon=\"üöÄ\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Custom CSS for modern look\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        font-size: 3rem;\n",
        "        font-weight: bold;\n",
        "        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
        "        -webkit-background-clip: text;\n",
        "        -webkit-text-fill-color: transparent;\n",
        "        text-align: center;\n",
        "        margin-bottom: 2rem;\n",
        "    }\n",
        "    .metric-card {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        padding: 1rem;\n",
        "        border-radius: 10px;\n",
        "        color: white;\n",
        "        text-align: center;\n",
        "        margin: 0.5rem 0;\n",
        "    }\n",
        "    .stButton > button {\n",
        "        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
        "        color: white;\n",
        "        border: none;\n",
        "        border-radius: 25px;\n",
        "        padding: 0.5rem 2rem;\n",
        "        font-weight: bold;\n",
        "        transition: all 0.3s ease;\n",
        "    }\n",
        "    .stButton > button:hover {\n",
        "        transform: translateY(-2px);\n",
        "        box_shadow: 0 5px 15px rgba(0,0,0,0.3);\n",
        "    }\n",
        "    .upload-section {\n",
        "        background: rgba(102, 126, 234, 0.1);\n",
        "        padding: 2rem;\n",
        "        border-radius: 15px;\n",
        "        border: 2px dashed #667eea;\n",
        "        text-align: center;\n",
        "        margin: 2rem 0;\n",
        "    }\n",
        "    .nav-button {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        color: white;\n",
        "        padding: 1rem 2rem;\n",
        "        border-radius: 10px;\n",
        "        text_decoration: none;\n",
        "        display: inline-block;\n",
        "        margin: 0.5rem;\n",
        "        transition: all 0.3s ease;\n",
        "    }\n",
        "    .nav-button:hover {\n",
        "        transform: translateY(-2px);\n",
        "        box_shadow: 0 5px 15px rgba(0,0,0,0.3);\n",
        "    }\n",
        "    .reinforcement-card {\n",
        "        background: linear_gradient(135deg, #ff6b6b 0%, #ee5a24 100%);\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 15px;\n",
        "        color: white;\n",
        "        margin: 1rem 0;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Initialize session state\n",
        "if 'current_page' not in st.session_state:\n",
        "    st.session_state.current_page = 'main'\n",
        "if 'uploaded_data' not in st.session_state:\n",
        "    st.session_state.uploaded_data = None\n",
        "if 'models_trained' not in st.session_state:\n",
        "    st.session_state.models_trained = False\n",
        "if 'reinforcement_data' not in st.session_state:\n",
        "    st.session_state.reinforcement_data = {}\n",
        "\n",
        "@st.cache_data\n",
        "def load_and_process_data(file):\n",
        "    \"\"\"Load and process the uploaded data with enhanced column detection\"\"\"\n",
        "    try:\n",
        "        if file.name.endswith('.csv'):\n",
        "            df = pd.read_csv(file)\n",
        "        else:\n",
        "            df = pd.read_excel(file)\n",
        "\n",
        "        # Clean column names\n",
        "        df.columns = df.columns.str.strip()\n",
        "\n",
        "        # Enhanced column detection for the new CSV format\n",
        "        date_col = None\n",
        "        product_col = None\n",
        "        sales_per_unit_col = None\n",
        "        quantity_col = None\n",
        "        total_sales_col = None\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_lower = col.lower()\n",
        "            if 'date' in col_lower or 'order' in col_lower:\n",
        "                date_col = col\n",
        "            elif 'product' in col_lower or 'category' in col_lower:\n",
        "                product_col = col\n",
        "            elif 'sales per unit' in col_lower or 'price' in col_lower:\n",
        "                sales_per_unit_col = col\n",
        "            elif 'quantity' in col_lower or 'qty' in col_lower:\n",
        "                quantity_col = col\n",
        "            elif 'total sales' in col_lower or 'total' in col_lower:\n",
        "                total_sales_col = col\n",
        "\n",
        "        # Fallback column detection\n",
        "        if date_col is None:\n",
        "            for col in df.columns:\n",
        "                if 'date' in col.lower():\n",
        "                    date_col = col\n",
        "                    break\n",
        "\n",
        "        if product_col is None:\n",
        "            for col in df.columns:\n",
        "                if 'product' in col.lower() or 'category' in col.lower():\n",
        "                    product_col = col\n",
        "                    break\n",
        "\n",
        "        if total_sales_col is None:\n",
        "            for col in df.columns:\n",
        "                if 'total' in col.lower():\n",
        "                    total_sales_col = col\n",
        "                    break\n",
        "\n",
        "        if quantity_col is None:\n",
        "            for col in df.columns:\n",
        "                if 'quantity' in col.lower():\n",
        "                    quantity_col = col\n",
        "                    break\n",
        "\n",
        "        if sales_per_unit_col is None:\n",
        "            for col in df.columns:\n",
        "                if 'sales per unit' in col.lower():\n",
        "                    sales_per_unit_col = col\n",
        "                    break\n",
        "\n",
        "        # Validate required columns\n",
        "        required_cols = [date_col, product_col, total_sales_col]\n",
        "        if any(col is None for col in required_cols):\n",
        "            st.error(\"Could not detect required columns. Please ensure your file has: Date, Product Category, and Total Sales columns.\")\n",
        "            st.error(f\"Available columns: {list(df.columns)}\")\n",
        "            return None\n",
        "\n",
        "        # Parse dates\n",
        "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "        df = df.dropna(subset=[date_col])\n",
        "\n",
        "        # Convert numeric columns\n",
        "        if total_sales_col:\n",
        "            df[total_sales_col] = pd.to_numeric(df[total_sales_col], errors='coerce')\n",
        "        if quantity_col:\n",
        "            df[quantity_col] = pd.to_numeric(df[quantity_col], errors='coerce')\n",
        "        if sales_per_unit_col:\n",
        "            df[sales_per_unit_col] = pd.to_numeric(df[sales_per_unit_col], errors='coerce')\n",
        "\n",
        "        # Rename columns for consistency\n",
        "        column_mapping = {\n",
        "            date_col: 'date',\n",
        "            product_col: 'product',\n",
        "            total_sales_col: 'total_sales'\n",
        "        }\n",
        "\n",
        "        if quantity_col:\n",
        "            column_mapping[quantity_col] = 'quantity'\n",
        "        if sales_per_unit_col:\n",
        "            column_mapping[sales_per_unit_col] = 'sales_per_unit'\n",
        "\n",
        "        df = df.rename(columns=column_mapping)\n",
        "\n",
        "        # Sort by date\n",
        "        df = df.sort_values('date')\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def create_advanced_regression_models():\n",
        "    \"\"\"Create multiple regression models for ensemble prediction\"\"\"\n",
        "    models = {\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Ridge Regression': Ridge(alpha=1.0),\n",
        "        'Lasso Regression': Lasso(alpha=0.1),\n",
        "        'SVR': SVR(kernel='rbf', C=100, gamma='scale')\n",
        "    }\n",
        "    return models\n",
        "\n",
        "def create_lstm_model(input_shape):\n",
        "    \"\"\"Create advanced LSTM model for time series prediction\"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        LSTM(128, return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        LSTM(32, return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def prepare_features_for_regression(df, product):\n",
        "    \"\"\"Prepare features for regression models\"\"\"\n",
        "    product_data = df[df['product'] == product].copy()\n",
        "\n",
        "    if len(product_data) < 10:\n",
        "        return None, None, None\n",
        "\n",
        "    # Create time-based features\n",
        "    product_data['year'] = product_data['date'].dt.year\n",
        "    product_data['month'] = product_data['date'].dt.month\n",
        "    product_data['day'] = product_data['date'].dt.day\n",
        "    product_data['day_of_week'] = product_data['date'].dt.dayofweek\n",
        "    product_data['quarter'] = product_data['date'].dt.quarter\n",
        "\n",
        "    # Create lag features\n",
        "    product_data = product_data.sort_values('date')\n",
        "    product_data['sales_lag_1'] = product_data['total_sales'].shift(1)\n",
        "    product_data['sales_lag_2'] = product_data['total_sales'].shift(2)\n",
        "    product_data['sales_lag_3'] = product_data['total_sales'].shift(3)\n",
        "\n",
        "    # Create rolling features\n",
        "    product_data['sales_rolling_mean_3'] = product_data['total_sales'].rolling(window=3).mean()\n",
        "    product_data['sales_rolling_mean_7'] = product_data['total_sales'].rolling(window=7).mean()\n",
        "    product_data['sales_rolling_std_3'] = product_data['total_sales'].rolling(window=3).std()\n",
        "\n",
        "    # Drop NaN values\n",
        "    product_data = product_data.dropna()\n",
        "\n",
        "    if len(product_data) < 5:\n",
        "        return None, None, None\n",
        "\n",
        "    # Prepare features\n",
        "    feature_columns = ['year', 'month', 'day', 'day_of_week', 'quarter',\n",
        "                      'sales_lag_1', 'sales_lag_2', 'sales_lag_3',\n",
        "                      'sales_rolling_mean_3', 'sales_rolling_mean_7', 'sales_rolling_std_3']\n",
        "\n",
        "    # Add quantity and sales_per_unit if available\n",
        "    if 'quantity' in product_data.columns:\n",
        "        feature_columns.append('quantity')\n",
        "    if 'sales_per_unit' in product_data.columns:\n",
        "        feature_columns.append('sales_per_unit')\n",
        "\n",
        "    X = product_data[feature_columns]\n",
        "    y = product_data['total_sales']\n",
        "\n",
        "    return X, y, product_data\n",
        "\n",
        "def train_ensemble_models(X, y, product):\n",
        "    \"\"\"Train multiple regression models and return ensemble predictions\"\"\"\n",
        "    models = create_advanced_regression_models()\n",
        "    trained_models = {}\n",
        "    predictions = {}\n",
        "    scores = {}\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            # Train model\n",
        "            model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            trained_models[name] = model\n",
        "            predictions[name] = y_pred\n",
        "            scores[name] = {\n",
        "                'MSE': mse,\n",
        "                'MAE': mae,\n",
        "                'R2': r2,\n",
        "                'RMSE': np.sqrt(mse)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Error training {name} for {product}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return trained_models, predictions, scores, scaler\n",
        "\n",
        "def predict_future_regression(trained_models, scaler, last_data, steps=6):\n",
        "    \"\"\"Predict future values using ensemble models\"\"\"\n",
        "    predictions = {}\n",
        "\n",
        "    for name, model in trained_models.items():\n",
        "        try:\n",
        "            future_predictions = []\n",
        "            current_data = last_data.copy()\n",
        "\n",
        "            for _ in range(steps):\n",
        "                # Prepare features for next prediction\n",
        "                features = current_data.reshape(1, -1)\n",
        "                features_scaled = scaler.transform(features)\n",
        "\n",
        "                # Predict next value\n",
        "                next_pred = model.predict(features_scaled)[0]\n",
        "                future_predictions.append(next_pred)\n",
        "\n",
        "                # Update features for next iteration (simplified)\n",
        "                # In a real scenario, you'd need to update all time-based features\n",
        "                current_data[0] = next_pred  # Update the target variable\n",
        "\n",
        "            predictions[name] = future_predictions\n",
        "\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Error predicting with {name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def create_reinforcement_learning_agent():\n",
        "    \"\"\"Create a simple reinforcement learning agent for business optimization\"\"\"\n",
        "    class BusinessRLAgent:\n",
        "        def __init__(self, n_actions=5):\n",
        "            self.n_actions = n_actions\n",
        "            self.q_table = {}\n",
        "            self.learning_rate = 0.1\n",
        "            self.discount_factor = 0.95\n",
        "            self.epsilon = 0.1\n",
        "\n",
        "        def get_state_key(self, state):\n",
        "            return str(state)\n",
        "\n",
        "        def get_action(self, state):\n",
        "            state_key = self.get_state_key(state)\n",
        "\n",
        "            if state_key not in self.q_table:\n",
        "                self.q_table[state_key] = np.zeros(self.n_actions)\n",
        "\n",
        "            # Epsilon-greedy strategy\n",
        "            if np.random.random() < self.epsilon:\n",
        "                return np.random.randint(self.n_actions)\n",
        "            else:\n",
        "                return np.argmax(self.q_table[state_key])\n",
        "\n",
        "        def update(self, state, action, reward, next_state):\n",
        "            state_key = self.get_state_key(state)\n",
        "            next_state_key = self.get_state_key(next_state)\n",
        "\n",
        "            if state_key not in self.q_table:\n",
        "                self.q_table[state_key] = np.zeros(self.n_actions)\n",
        "            if next_state_key not in self.q_table:\n",
        "                self.q_table[next_state_key] = np.zeros(self.n_actions)\n",
        "\n",
        "            # Q-learning update\n",
        "            current_q = self.q_table[state_key][action]\n",
        "            max_next_q = np.max(self.q_table[next_state_key])\n",
        "            new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
        "            self.q_table[state_key][action] = new_q\n",
        "\n",
        "            def sidebar_nav(df: pd.DataFrame | None):\n",
        "              with st.sidebar:\n",
        "                pass # Placeholder for sidebar navigation\n",
        "\n",
        "    return BusinessRLAgent()\n",
        "\n",
        "def generate_business_recommendations(df, product_models, predictions):\n",
        "    \"\"\"Generate business recommendations using reinforcement learning\"\"\"\n",
        "    recommendations = {}\n",
        "\n",
        "    for product in df['product'].unique():\n",
        "        if product in product_models:\n",
        "            product_data = df[df['product'] == product]\n",
        "\n",
        "            # Calculate business metrics\n",
        "            avg_sales = product_data['total_sales'].mean()\n",
        "            sales_volatility = product_data['total_sales'].std()\n",
        "            total_revenue = product_data['total_sales'].sum()\n",
        "\n",
        "            # Create RL agent\n",
        "            agent = create_reinforcement_learning_agent()\n",
        "\n",
        "            # Simulate business scenarios\n",
        "            scenarios = []\n",
        "            for _ in range(100):\n",
        "                # Define state based on current metrics\n",
        "                state = [\n",
        "                    int(avg_sales > product_data['total_sales'].median()),\n",
        "                    int(sales_volatility > product_data['total_sales'].std()),\n",
        "                    int(total_revenue > product_data['total_sales'].sum() * 0.8)\n",
        "                ]\n",
        "\n",
        "                action = agent.get_action(state)\n",
        "\n",
        "                # Calculate reward based on predicted performance\n",
        "                if product in predictions:\n",
        "                    predicted_avg = np.mean(predictions[product]['ensemble'])\n",
        "                    reward = 1 if predicted_avg > avg_sales else -0.5\n",
        "                else:\n",
        "                    reward = 0\n",
        "\n",
        "                # Update agent\n",
        "                next_state = state.copy()\n",
        "                agent.update(state, action, reward, next_state)\n",
        "\n",
        "                scenarios.append({\n",
        "                    'action': action,\n",
        "                    'reward': reward,\n",
        "                    'state': state\n",
        "                })\n",
        "\n",
        "            # Generate recommendations based on best actions\n",
        "            best_action = np.argmax([s['reward'] for s in scenarios])\n",
        "\n",
        "            action_descriptions = {\n",
        "                0: \"Increase marketing budget\",\n",
        "                1: \"Optimize pricing strategy\",\n",
        "                2: \"Improve inventory management\",\n",
        "                3: \"Enhance customer service\",\n",
        "                4: \"Expand product line\"\n",
        "            }\n",
        "\n",
        "            recommendations[product] = {\n",
        "                'recommended_action': action_descriptions[best_action],\n",
        "                'confidence_score': np.mean([s['reward'] for s in scenarios]),\n",
        "                'current_avg_sales': avg_sales,\n",
        "                'predicted_improvement': predicted_avg - avg_sales if product in predictions else 0,\n",
        "                'risk_level': 'High' if sales_volatility > avg_sales * 0.5 else 'Low'\n",
        "            }\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "def main_page():\n",
        "    \"\"\"Main dashboard page\"\"\"\n",
        "    st.markdown('<h1 class=\"main-header\">üöÄ Advanced Sales Prediction AI</h1>', unsafe_allow_html=True)\n",
        "    st.markdown(\"### Transform your transaction data into actionable sales insights with Advanced AI & Reinforcement Learning\")\n",
        "\n",
        "    # Navigation buttons\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "    with col1:\n",
        "        if st.button(\"üìä Historical Sales Prediction\", type=\"primary\", use_container_width=True):\n",
        "            st.session_state.current_page = 'historical'\n",
        "            st.rerun()\n",
        "\n",
        "    with col2:\n",
        "        if st.button(\"ü§ñ Reinforcement Learning\", type=\"primary\", use_container_width=True):\n",
        "            st.session_state.current_page = 'reinforcement'\n",
        "            st.rerun()\n",
        "\n",
        "    with col3:\n",
        "        if st.button(\"üìà Business Analytics\", type=\"primary\", use_container_width=True):\n",
        "            st.session_state.current_page = 'analytics'\n",
        "            st.rerun()\n",
        "\n",
        "    # File upload section\n",
        "    st.markdown('<div class=\"upload-section\">', unsafe_allow_html=True)\n",
        "    st.markdown(\"### üìÅ Upload Your Transaction Data\")\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Choose a CSV or Excel file\",\n",
        "        type=['csv', 'xlsx', 'xls'],\n",
        "        help=\"Upload your transaction data with columns: Order Date, Product Category, Sales per Unit, Quantity, Total Sales\"\n",
        "    )\n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        with st.spinner(\"üîÑ Loading and processing your data...\"):\n",
        "            df = load_and_process_data(uploaded_file)\n",
        "            st.session_state.uploaded_data = df\n",
        "\n",
        "        if df is not None:\n",
        "            st.success(f\"‚úÖ Data loaded successfully! Found {len(df)} transactions across {df['product'].nunique()} products\")\n",
        "\n",
        "            # Store data in session state\n",
        "            st.session_state.uploaded_data = df\n",
        "\n",
        "            # Data overview\n",
        "            col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "            with col1:\n",
        "                st.metric(\"Total Transactions\", len(df))\n",
        "\n",
        "            with col2:\n",
        "                st.metric(\"Unique Products\", df['product'].nunique())\n",
        "\n",
        "            with col3:\n",
        "                st.metric(\"Date Range\", f\"{df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            with col4:\n",
        "                st.metric(\"Total Revenue\", f\"${df['total_sales'].sum():,.0f}\")\n",
        "\n",
        "def historical_prediction_page():\n",
        "    \"\"\"Historical sales prediction page\"\"\"\n",
        "    st.markdown('<h1 class=\"main-header\">üìä Historical Sales Prediction</h1>', unsafe_allow_html=True)\n",
        "\n",
        "    # Back button\n",
        "    if st.button(\"‚Üê Back to Main Menu\"):\n",
        "        st.session_state.current_page = 'main'\n",
        "        st.rerun()\n",
        "\n",
        "    if st.session_state.uploaded_data is None:\n",
        "        st.warning(\"Please upload data from the main menu first.\")\n",
        "        return\n",
        "\n",
        "    df = st.session_state.uploaded_data\n",
        "\n",
        "    # Product selection\n",
        "    st.markdown(\"## üéØ Select Products for Advanced Analysis\")\n",
        "    products = df['product'].unique()\n",
        "    selected_products = st.multiselect(\n",
        "        \"Choose products to analyze\",\n",
        "        products,\n",
        "        default=products[:min(5, len(products))],\n",
        "        help=\"Select products for advanced regression analysis and prediction\"\n",
        "    )\n",
        "\n",
        "    if selected_products:\n",
        "        # Prediction period selection\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            prediction_period = st.selectbox(\n",
        "                \"Select Prediction Period\",\n",
        "                [\"1 Month\", \"3 Months\", \"6 Months\", \"1 Year\"],\n",
        "                index=2\n",
        "            )\n",
        "\n",
        "        with col2:\n",
        "            show_ensemble = st.checkbox(\n",
        "                \"Show Ensemble Model Comparison\",\n",
        "                value=True\n",
        "            )\n",
        "\n",
        "        if st.button(\"üöÄ Train Advanced AI Models\", type=\"primary\"):\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "\n",
        "            product_models = {}\n",
        "            all_predictions = {}\n",
        "            all_scores = {}\n",
        "\n",
        "            for i, product in enumerate(selected_products):\n",
        "                status_text.text(f\"Training advanced models for {product}...\")\n",
        "                progress_bar.progress((i + 1) / len(selected_products))\n",
        "\n",
        "                # Prepare features\n",
        "                X, y, product_data = prepare_features_for_regression(df, product)\n",
        "\n",
        "                if X is not None:\n",
        "                    # Train ensemble models\n",
        "                    trained_models, predictions, scores, scaler = train_ensemble_models(X, y, product)\n",
        "\n",
        "                    if trained_models:\n",
        "                        product_models[product] = {\n",
        "                            'models': trained_models,\n",
        "                            'scaler': scaler,\n",
        "                            'data': product_data\n",
        "                        }\n",
        "\n",
        "                        all_scores[product] = scores\n",
        "\n",
        "                        # Predict future values\n",
        "                        last_data = X.iloc[-1].values\n",
        "                        future_predictions = predict_future_regression(trained_models, scaler, last_data, steps=6)\n",
        "\n",
        "                        # Calculate ensemble prediction\n",
        "                        if future_predictions:\n",
        "                            ensemble_predictions = []\n",
        "                            for step in range(6):\n",
        "                                step_predictions = [pred[step] for pred in future_predictions.values() if len(pred) > step]\n",
        "                                if step_predictions:\n",
        "                                    ensemble_predictions.append(np.mean(step_predictions))\n",
        "\n",
        "                            all_predictions[product] = {\n",
        "                                'individual': future_predictions,\n",
        "                                'ensemble': ensemble_predictions\n",
        "                            }\n",
        "\n",
        "            progress_bar.progress(1.0)\n",
        "            status_text.text(\"‚úÖ Advanced AI models trained successfully!\")\n",
        "\n",
        "            # Store results in session state\n",
        "            st.session_state.product_models = product_models\n",
        "            st.session_state.all_predictions = all_predictions\n",
        "            st.session_state.all_scores = all_scores\n",
        "            st.session_state.models_trained = True\n",
        "\n",
        "            # Display results\n",
        "            display_prediction_results(all_predictions, all_scores, show_ensemble)\n",
        "\n",
        "def display_prediction_results(predictions, scores, show_ensemble):\n",
        "    \"\"\"Display prediction results with advanced visualizations\"\"\"\n",
        "    if not predictions:\n",
        "        st.warning(\"No predictions available. Please check your data and try again.\")\n",
        "        return\n",
        "\n",
        "    st.markdown(\"## üìà Advanced Prediction Results\")\n",
        "\n",
        "    # Create tabs for different views\n",
        "    tab1, tab2, tab3 = st.tabs([\"üéØ Predictions\", \"üìä Model Performance\", \"üìà Ensemble Analysis\"])\n",
        "\n",
        "    with tab1:\n",
        "        st.markdown(\"### Future Sales Predictions\")\n",
        "\n",
        "        # Create prediction visualization\n",
        "        fig = go.Figure()\n",
        "\n",
        "        for product, pred_data in predictions.items():\n",
        "            if 'ensemble' in pred_data:\n",
        "                # Historical data\n",
        "                historical_data = st.session_state.product_models[product]['data']\n",
        "\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=historical_data['date'],\n",
        "                    y=historical_data['total_sales'],\n",
        "                    mode='lines+markers',\n",
        "                    name=f\"{product} (Historical)\",\n",
        "                    line=dict(width=2)\n",
        "                ))\n",
        "\n",
        "                # Ensemble predictions\n",
        "                future_dates = pd.date_range(\n",
        "                    start=historical_data['date'].iloc[-1] + pd.DateOffset(days=1),\n",
        "                    periods=len(pred_data['ensemble']),\n",
        "                    freq='M'\n",
        "                )\n",
        "\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=future_dates,\n",
        "                    y=pred_data['ensemble'],\n",
        "                    mode='lines+markers',\n",
        "                    name=f\"{product} (Predicted)\",\n",
        "                    line=dict(width=2, dash='dash'),\n",
        "                    marker=dict(symbol='diamond')\n",
        "                ))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=\"Historical Sales vs AI Predictions\",\n",
        "            xaxis_title=\"Date\",\n",
        "            yaxis_title=\"Sales Amount\",\n",
        "            height=600,\n",
        "            showlegend=True\n",
        "        )\n",
        "\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    with tab2:\n",
        "        st.markdown(\"### Model Performance Comparison\")\n",
        "\n",
        "        if show_ensemble and scores:\n",
        "            # Create performance comparison table\n",
        "            performance_data = []\n",
        "\n",
        "            for product, product_scores in scores.items():\n",
        "                for model_name, metrics in product_scores.items():\n",
        "                    performance_data.append({\n",
        "                        'Product': product,\n",
        "                        'Model': model_name,\n",
        "                        'R¬≤ Score': f\"{metrics['R2']:.3f}\",\n",
        "                        'RMSE': f\"${metrics['RMSE']:.2f}\",\n",
        "                        'MAE': f\"${metrics['MAE']:.2f}\"\n",
        "                    })\n",
        "\n",
        "            performance_df = pd.DataFrame(performance_data)\n",
        "            st.dataframe(performance_df, use_container_width=True)\n",
        "\n",
        "            # Performance heatmap\n",
        "            pivot_df = performance_df.pivot(index='Product', columns='Model', values='R¬≤ Score')\n",
        "            pivot_df = pivot_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "            fig = px.imshow(\n",
        "                pivot_df,\n",
        "                title=\"Model Performance Heatmap (R¬≤ Scores)\",\n",
        "                color_continuous_scale='RdYlGn'\n",
        "            )\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    with tab3:\n",
        "        st.markdown(\"### Ensemble Model Analysis\")\n",
        "\n",
        "        if predictions:\n",
        "            # Compare individual models vs ensemble\n",
        "            fig = go.Figure()\n",
        "\n",
        "            for product, pred_data in predictions.items():\n",
        "                if 'individual' in pred_data:\n",
        "                    # Plot individual model predictions\n",
        "                    for model_name, model_preds in pred_data['individual'].items():\n",
        "                        if len(model_preds) > 0:\n",
        "                            fig.add_trace(go.Scatter(\n",
        "                                x=list(range(len(model_preds))),\n",
        "                                y=model_preds,\n",
        "                                mode='lines+markers',\n",
        "                                name=f\"{product} - {model_name}\",\n",
        "                                line=dict(width=1, dash='dot')\n",
        "                            ))\n",
        "\n",
        "                    # Plot ensemble prediction\n",
        "                    if 'ensemble' in pred_data:\n",
        "                        fig.add_trace(go.Scatter(\n",
        "                            x=list(range(len(pred_data['ensemble']))),\n",
        "                            y=pred_data['ensemble'],\n",
        "                            mode='lines+markers',\n",
        "                            name=f\"{product} - Ensemble\",\n",
        "                            line=dict(width=3),\n",
        "                            marker=dict(symbol='diamond', size=8)\n",
        "                        ))\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=\"Individual Models vs Ensemble Predictions\",\n",
        "                xaxis_title=\"Prediction Steps\",\n",
        "                yaxis_title=\"Predicted Sales\",\n",
        "                height=500,\n",
        "                showlegend=True\n",
        "            )\n",
        "\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "def reinforcement_learning_page():\n",
        "    \"\"\"Reinforcement learning for business optimization\"\"\"\n",
        "    st.markdown('<h1 class=\"main-header\">ü§ñ Reinforcement Learning Business Optimization</h1>', unsafe_allow_html=True)\n",
        "\n",
        "    # Back button\n",
        "    if st.button(\"‚Üê Back to Main Menu\"):\n",
        "        st.session_state.current_page = 'main'\n",
        "        st.rerun()\n",
        "\n",
        "    if st.session_state.uploaded_data is None:\n",
        "        st.warning(\"Please upload data from the main menu first.\")\n",
        "        return\n",
        "\n",
        "    df = st.session_state.uploaded_data\n",
        "\n",
        "    st.markdown(\"## üéØ Business Optimization with AI\")\n",
        "    st.markdown(\"This page uses reinforcement learning to provide business recommendations for improving sales performance.\")\n",
        "\n",
        "    # Product selection for RL analysis\n",
        "    products = df['product'].unique()\n",
        "    selected_products = st.multiselect(\n",
        "        \"Choose products for business optimization\",\n",
        "        products,\n",
        "        default=products[:min(3, len(products))],\n",
        "        help=\"Select products to analyze with reinforcement learning\"\n",
        "    )\n",
        "\n",
        "    if selected_products and st.button(\"üöÄ Generate Business Recommendations\", type=\"primary\"):\n",
        "        with st.spinner(\"ü§ñ Training reinforcement learning agent...\"):\n",
        "            # Generate recommendations\n",
        "            recommendations = generate_business_recommendations(df,\n",
        "                                                           st.session_state.get('product_models', {}),\n",
        "                                                           st.session_state.get('all_predictions', {}))\n",
        "\n",
        "            st.session_state.reinforcement_data = recommendations\n",
        "\n",
        "            # Display recommendations\n",
        "            display_business_recommendations(recommendations, selected_products)\n",
        "\n",
        "def display_business_recommendations(recommendations, selected_products):\n",
        "    \"\"\"Display business recommendations with visualizations\"\"\"\n",
        "    st.markdown(\"## üí° AI Business Recommendations\")\n",
        "\n",
        "    # Create tabs for different recommendation views\n",
        "    tab1, tab2, tab3 = st.tabs([\"üéØ Recommendations\", \"üìä Performance Analysis\", \"üöÄ Action Plan\"])\n",
        "\n",
        "    with tab1:\n",
        "        st.markdown(\"### Product-Specific Recommendations\")\n",
        "\n",
        "        for product in selected_products:\n",
        "            if product in recommendations:\n",
        "                rec = recommendations[product]\n",
        "\n",
        "                with st.container():\n",
        "                    st.markdown(f\"### {product}\")\n",
        "\n",
        "                    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "                    with col1:\n",
        "                        st.metric(\"Current Avg Sales\", f\"${rec['current_avg_sales']:.2f}\")\n",
        "\n",
        "                    with col2:\n",
        "                        st.metric(\"Predicted Improvement\", f\"${rec['predicted_improvement']:.2f}\")\n",
        "\n",
        "                    with col3:\n",
        "                        st.metric(\"Risk Level\", rec['risk_level'])\n",
        "\n",
        "                    # Recommendation card\n",
        "                    st.markdown(f\"\"\"\n",
        "                    <div class=\"reinforcement-card\">\n",
        "                        <h4>üéØ Recommended Action</h4>\n",
        "                        <p><strong>{rec['recommended_action']}</strong></p>\n",
        "                        <p>Confidence Score: {rec['confidence_score']:.2f}</p>\n",
        "                    </div>\n",
        "                    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    with tab2:\n",
        "        st.markdown(\"### Performance Analysis\")\n",
        "\n",
        "        if recommendations:\n",
        "            # Create performance comparison chart\n",
        "            products = list(recommendations.keys())\n",
        "            current_sales = [recommendations[p]['current_avg_sales'] for p in products]\n",
        "            predicted_improvements = [recommendations[p]['predicted_improvement'] for p in products]\n",
        "            confidence_scores = [recommendations[p]['confidence_score'] for p in products]\n",
        "\n",
        "            fig = go.Figure()\n",
        "\n",
        "            fig.add_trace(go.Bar(\n",
        "                x=products,\n",
        "                y=current_sales,\n",
        "                name='Current Avg Sales',\n",
        "                marker_color='lightblue'\n",
        "            ))\n",
        "\n",
        "            fig.add_trace(go.Bar(\n",
        "                x=products,\n",
        "                y=predicted_improvements,\n",
        "                name='Predicted Improvement',\n",
        "                marker_color='lightgreen'\n",
        "            ))\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=\"Current vs Predicted Performance\",\n",
        "                xaxis_title=\"Products\",\n",
        "                yaxis_title=\"Sales Amount ($)\",\n",
        "                barmode='group',\n",
        "                height=500\n",
        "            )\n",
        "\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "            # Confidence scores\n",
        "            fig2 = px.bar(\n",
        "                x=products,\n",
        "                y=confidence_scores,\n",
        "                title=\"AI Confidence Scores\",\n",
        "                labels={'x': 'Products', 'y': 'Confidence Score'}\n",
        "            )\n",
        "\n",
        "            st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "    with tab3:\n",
        "        st.markdown(\"### üöÄ Action Plan\")\n",
        "\n",
        "        if recommendations:\n",
        "            st.markdown(\"### Prioritized Action Items\")\n",
        "\n",
        "            # Sort recommendations by confidence score\n",
        "            sorted_recs = sorted(recommendations.items(),\n",
        "                               key=lambda x: x[1]['confidence_score'],\n",
        "                               reverse=True)\n",
        "\n",
        "            for i, (product, rec) in enumerate(sorted_recs, 1):\n",
        "                st.markdown(f\"\"\"\n",
        "                **{i}. {product}**\n",
        "                - **Action**: {rec['recommended_action']}\n",
        "                - **Expected Impact**: ${rec['predicted_improvement']:.2f} improvement\n",
        "                - **Confidence**: {rec['confidence_score']:.2f}\n",
        "                - **Risk Level**: {rec['risk_level']}\n",
        "                \"\"\")\n",
        "\n",
        "            # Summary metrics\n",
        "            st.markdown(\"### üìä Summary\")\n",
        "\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "\n",
        "            with col1:\n",
        "                total_improvement = sum(rec['predicted_improvement'] for rec in recommendations.values())\n",
        "                st.metric(\"Total Predicted Improvement\", f\"${total_improvement:.2f}\")\n",
        "\n",
        "            with col2:\n",
        "                avg_confidence = np.mean([rec['confidence_score'] for rec in recommendations.values()])\n",
        "                st.metric(\"Average Confidence\", f\"{avg_confidence:.2f}\")\n",
        "\n",
        "            with col3:\n",
        "                high_risk_count = sum(1 for rec in recommendations.values() if rec['risk_level'] == 'High')\n",
        "                st.metric(\"High Risk Products\", high_risk_count)\n",
        "\n",
        "def analytics_page():\n",
        "    \"\"\"Business analytics page\"\"\"\n",
        "    st.markdown('<h1 class=\"main-header\">üìà Business Analytics Dashboard</h1>', unsafe_allow_html=True)\n",
        "\n",
        "    # Back button\n",
        "    if st.button(\"‚Üê Back to Main Menu\"):\n",
        "        st.session_state.current_page = 'main'\n",
        "        st.rerun()\n",
        "\n",
        "    if st.session_state.uploaded_data is None:\n",
        "        st.warning(\"Please upload data from the main menu first.\")\n",
        "        return\n",
        "\n",
        "    df = st.session_state.uploaded_data\n",
        "\n",
        "    st.markdown(\"## üìä Advanced Business Analytics\")\n",
        "\n",
        "    # Create tabs for different analytics\n",
        "    tab1, tab2, tab3 = st.tabs([\"üìà Sales Trends\", \"üéØ Product Analysis\", \"üìä Revenue Insights\"])\n",
        "\n",
        "    with tab1:\n",
        "        st.markdown(\"### Sales Trends Analysis\")\n",
        "\n",
        "        # Monthly sales trend\n",
        "        monthly_sales = df.groupby(df['date'].dt.to_period('M'))['total_sales'].sum()\n",
        "\n",
        "        fig = px.line(\n",
        "            x=monthly_sales.index.astype(str),\n",
        "            y=monthly_sales.values,\n",
        "            title=\"Monthly Sales Trend\",\n",
        "            labels={'x': 'Month', 'y': 'Total Sales'}\n",
        "        )\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "        # Product performance over time\n",
        "        product_monthly = df.groupby([df['date'].dt.to_period('M'), 'product'])['total_sales'].sum().reset_index()\n",
        "        product_monthly['date'] = product_monthly['date'].astype(str)\n",
        "\n",
        "        fig2 = px.line(\n",
        "            product_monthly,\n",
        "            x='date',\n",
        "            y='total_sales',\n",
        "            color='product',\n",
        "            title=\"Product Sales Trends\"\n",
        "        )\n",
        "        st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "    with tab2:\n",
        "        st.markdown(\"### Product Performance Analysis\")\n",
        "\n",
        "        # Top products by revenue\n",
        "        top_products = df.groupby('product')['total_sales'].sum().sort_values(ascending=False)\n",
        "\n",
        "        fig = px.bar(\n",
        "            x=top_products.values,\n",
        "            y=top_products.index,\n",
        "            orientation='h',\n",
        "            title=\"Top Products by Revenue\"\n",
        "        )\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "        # Product sales distribution\n",
        "        fig2 = px.box(\n",
        "            df,\n",
        "            x='product',\n",
        "            y='total_sales',\n",
        "            title=\"Sales Distribution by Product\"\n",
        "        )\n",
        "        st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "    with tab3:\n",
        "        st.markdown(\"### Revenue Insights\")\n",
        "\n",
        "        # Revenue metrics\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "        with col1:\n",
        "            total_revenue = df['total_sales'].sum()\n",
        "            st.metric(\"Total Revenue\", f\"${total_revenue:,.0f}\")\n",
        "\n",
        "        with col2:\n",
        "            avg_order_value = df['total_sales'].mean()\n",
        "            st.metric(\"Average Order Value\", f\"${avg_order_value:.2f}\")\n",
        "\n",
        "        with col3:\n",
        "            total_orders = len(df)\n",
        "            st.metric(\"Total Orders\", total_orders)\n",
        "\n",
        "        with col4:\n",
        "            unique_customers = df['product'].nunique()  # Assuming product as proxy for customer segments\n",
        "            st.metric(\"Product Categories\", unique_customers)\n",
        "\n",
        "        # Revenue by product category\n",
        "        revenue_by_product = df.groupby('product')['total_sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
        "        revenue_by_product.columns = ['Product', 'Total Revenue', 'Average Order Value', 'Number of Orders']\n",
        "\n",
        "        st.dataframe(revenue_by_product, use_container_width=True)\n",
        "\n",
        "# Main application logic\n",
        "def main():\n",
        "    # Page routing\n",
        "    if st.session_state.current_page == 'main':\n",
        "        main_page()\n",
        "    elif st.session_state.current_page == 'historical':\n",
        "        historical_prediction_page()\n",
        "    elif st.session_state.current_page == 'reinforcement':\n",
        "        reinforcement_learning_page()\n",
        "    elif st.session_state.current_page == 'analytics':\n",
        "        analytics_page()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "# [Rest of your Streamlit code goes here...]\n",
        "# Paste ALL your Streamlit code from your original file here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below will run the Streamlit app and create a public URL\n",
        "!streamlit run app.py &>/dev/null&\n",
        "!./ngrok http 8501\n",
        "\n",
        "# Get the public URL\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url']\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBZ7rcFK8I66",
        "outputId": "43b6ba6d-2fe6-42af-f8d9-50e6c00f9db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage of ngrok requires a verified account and authtoken.\n",
            "\n",
            "Sign up for an account: https://dashboard.ngrok.com/signup\n",
            "Install your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\r\n",
            "\r\n",
            "ERR_NGROK_4018\r\n",
            "\n",
            "  File \"<string>\", line 1\n",
            "    import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url']\n",
            "                           ^\n",
            "SyntaxError: '(' was never closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfsthElO-Wjc",
        "outputId": "2de95d56-ffd0-447b-824a-a2d73ee742d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cd4L9kd2-rob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSXLt_fDxyt1",
        "outputId": "f349f564-97af-4a98-a90d-7e9cc8d36ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.31.94.136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app8.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg2lUtWB-f7s",
        "outputId": "cde004e0-7978-4d71-fc9e-082542f5c270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.31.94.136:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0Kyour url is: https://clear-paws-help.loca.lt\n",
            "2025-08-14 09:33:19.756958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755163999.787870   25181 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755163999.798218   25181 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755163999.822756   25181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755163999.822816   25181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755163999.822822   25181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755163999.822827   25181 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-14 09:33:19.830051: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}